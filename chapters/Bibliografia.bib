
@techreport{waltermire_forming_2015,
	address = {NIST IR 8085},
	title = {Forming {Common} {Platform} {Enumeration} ({CPE}) {Names} from {Software} {Identification} ({SWID}) {Tags}},
	url = {https://csrc.nist.gov/pubs/ir/8085/ipd},
	abstract = {This report provides an overview of the capabilities and usage of software identification (SWID) tags as part of a comprehensive software lifecycle. As instantiated in the International Organization for Standardization/International Electrotechnical Commission 19770-2 standard, SWID tags support numerous applications for software asset management and information security management. This report introduces SWID tags in an operational context, provides guidelines for the creation of interoperable SWID tags, and highlights key usage scenarios for which SWID tags are applicable.},
	language = {en},
	number = {NIST IR 8085},
	institution = {National Institute of Standards and Technology},
	author = {Waltermire, David and Cheikes, Brant},
	year = {2015},
	keywords = {NIST},
	file = {nistir_8085_draft.pdf:C\:\\Users\\glmqu\\Zotero\\storage\\EVHPEFG7\\nistir_8085_draft.pdf:application/pdf},
}

@techreport{waltermire_guidelines_2016,
	address = {NIST IR 8060},
	title = {Guidelines for the {Creation} of {Interoperable} {Software} {Identification} ({SWID}) {Tags}},
	url = {https://csrc.nist.gov/pubs/ir/8060/final},
	abstract = {This report provides an overview of the capabilities and usage of software identification (SWID) tags as part of a comprehensive software lifecycle. As instantiated in the International Organization for Standardization/International Electrotechnical Commission 19770-2 standard, SWID tags support numerous applications for software asset management and information security management. This report introduces SWID tags in an operational context, provides guidelines for the creation of interoperable SWID tags, and highlights key usage scenarios for which SWID tags are applicable.},
	language = {en},
	number = {NIST IR 8060},
	institution = {National Institute of Standards and Technology},
	author = {Waltermire, David and Cheikes, Brant and Feldman, Larry and Witte, Gregory},
	year = {2016},
	keywords = {NIST},
	file = {NIST.IR.8060.pdf:C\:\\Users\\glmqu\\Zotero\\storage\\EHVWFS45\\NIST.IR.8060.pdf:application/pdf},
}

@misc{aghaei_securebert_2022,
	title = {{SecureBERT}: {A} {Domain}-{Specific} {Language} {Model} for {Cybersecurity}},
	shorttitle = {{SecureBERT}},
	url = {http://arxiv.org/abs/2204.02685},
	doi = {10.48550/arXiv.2204.02685},
	abstract = {Natural Language Processing (NLP) has recently gained wide attention in cybersecurity, particularly in Cyber Threat Intelligence (CTI) and cyber automation. Increased connection and automation have revolutionized the world's economic and cultural infrastructures, while they have introduced risks in terms of cyber attacks. CTI is information that helps cybersecurity analysts make intelligent security decisions, that is often delivered in the form of natural language text, which must be transformed to machine readable format through an automated procedure before it can be used for automated security measures. This paper proposes SecureBERT, a cybersecurity language model capable of capturing text connotations in cybersecurity text (e.g., CTI) and therefore successful in automation for many critical cybersecurity tasks that would otherwise rely on human expertise and time-consuming manual efforts. SecureBERT has been trained using a large corpus of cybersecurity text.To make SecureBERT effective not just in retaining general English understanding, but also when applied to text with cybersecurity implications, we developed a customized tokenizer as well as a method to alter pre-trained weights. The SecureBERT is evaluated using the standard Masked Language Model (MLM) test as well as two additional standard NLP tasks. Our evaluation studies show that SecureBERT{\textbackslash}footnote\{{\textbackslash}url\{https://github.com/ehsanaghaei/SecureBERT\}\} outperforms existing similar models, confirming its capability for solving crucial NLP tasks in cybersecurity.},
	urldate = {2024-12-11},
	publisher = {arXiv},
	author = {Aghaei, Ehsan and Niu, Xi and Shadid, Waseem and Al-Shaer, Ehab},
	month = oct,
	year = {2022},
	note = {arXiv:2204.02685 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security},
	annote = {Comment: This is the initial draft of this work and it may contain errors and typos. The revised version has already been submitted to a venue},
	file = {Preprint PDF:C\:\\Users\\glmqu\\Zotero\\storage\\HY3HZBC7\\Aghaei et al. - 2022 - SecureBERT A Domain-Specific Language Model for Cybersecurity.pdf:application/pdf;Snapshot:C\:\\Users\\glmqu\\Zotero\\storage\\F27GBW55\\2204.html:text/html},
}

@inproceedings{joshi_extracting_2013,
	title = {Extracting {Cybersecurity} {Related} {Linked} {Data} from {Text}},
	url = {https://ieeexplore.ieee.org/document/6693525},
	doi = {10.1109/ICSC.2013.50},
	abstract = {The Web is typically our first source of information about new software vulnerabilities, exploits and cyber-attacks. Information is found in semi-structured vulnerability databases as well as in text from security bulletins, news reports, cyber security blogs and Internet chat rooms. It can be useful to cyber security systems if there is a way to recognize and extract relevant information and represent it as easily shared and integrated semantic data. We describe such an automatic framework that generates and publishes a RDF linked data representation of cyber security concepts and vulnerability descriptions extracted from the National Vulnerability Database and from text sources. A CRF-based system is used to identify cybersecurity-relatedentities, concepts and relations in text, which are then represented using custom ontologies for the cyber security domain and also mapped to objects in the DBpedia knowledge base. The resulting cyber security linked data collection can be used for many purposes, including automating early vulnerability identification, mitigation and prevention efforts.},
	urldate = {2024-12-11},
	booktitle = {2013 {IEEE} {Seventh} {International} {Conference} on {Semantic} {Computing}},
	author = {Joshi, Arnav and Lal, Ravendar and Finin, Tim and Joshi, Anupam},
	month = sep,
	year = {2013},
	keywords = {Computer crime, cybeesecurity, Data mining, information extraction, linked data, Ontologies, ontology, Resource description framework, Software},
	pages = {252--259},
	file = {Full Text PDF:C\:\\Users\\glmqu\\Zotero\\storage\\S5YQCXB5\\Joshi et al. - 2013 - Extracting Cybersecurity Related Linked Data from Text.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\glmqu\\Zotero\\storage\\8KQG3Z3A\\6693525.html:text/html},
}

@misc{bridges_automatic_2014,
	title = {Automatic {Labeling} for {Entity} {Extraction} in {Cyber} {Security}},
	url = {http://arxiv.org/abs/1308.4941},
	doi = {10.48550/arXiv.1308.4941},
	abstract = {Timely analysis of cyber-security information necessitates automated information extraction from unstructured text. While state-of-the-art extraction methods produce extremely accurate results, they require ample training data, which is generally unavailable for specialized applications, such as detecting security related entities; moreover, manual annotation of corpora is very costly and often not a viable solution. In response, we develop a very precise method to automatically label text from several data sources by leveraging related, domain-specific, structured data and provide public access to a corpus annotated with cyber-security entities. Next, we implement a Maximum Entropy Model trained with the average perceptron on a portion of our corpus (\${\textbackslash}sim\$750,000 words) and achieve near perfect precision, recall, and accuracy, with training times under 17 seconds.},
	urldate = {2024-12-11},
	publisher = {arXiv},
	author = {Bridges, Robert A. and Jones, Corinne L. and Iannacone, Michael D. and Testa, Kelly M. and Goodall, John R.},
	month = jun,
	year = {2014},
	note = {arXiv:1308.4941 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	annote = {Comment: 10 pages},
	file = {Preprint PDF:C\:\\Users\\glmqu\\Zotero\\storage\\9IXXN3EX\\Bridges et al. - 2014 - Automatic Labeling for Entity Extraction in Cyber Security.pdf:application/pdf;Snapshot:C\:\\Users\\glmqu\\Zotero\\storage\\DML77Q9D\\1308.html:text/html},
}

@article{kanakogi_tracing_2021,
	title = {Tracing {CVE} {Vulnerability} {Information} to {CAPEC} {Attack} {Patterns} {Using} {Natural} {Language} {Processing} {Techniques}},
	volume = {12},
	doi = {10.3390/info12080298},
	abstract = {For effective vulnerability management, vulnerability and attack information must be collected quickly and efficiently. A security knowledge repository can collect such information. The Common Vulnerabilities and Exposures (CVE) provides known vulnerabilities of products, while the Common Attack Pattern Enumeration and Classification (CAPEC) stores attack patterns, which are descriptions of common attributes and approaches employed by adversaries to exploit known weaknesses. Due to the fact that the information in these two repositories are not linked, identifying related CAPEC attack information from CVE vulnerability information is challenging. Currently, the related CAPEC-ID can be traced from the CVE-ID using Common Weakness Enumeration (CWE) in some but not all cases. Here, we propose a method to automatically trace the related CAPEC-IDs from CVE-ID using three similarity measures: TF–IDF, Universal Sentence Encoder (USE), and Sentence-BERT (SBERT). We prepared and used 58 CVE-IDs as test input data. Then, we tested whether we could trace CAPEC-IDs related to each of the 58 CVE-IDs. Additionally, we experimentally confirm that TF–IDF is the best similarity measure, as it traced 48 of the 58 CVE-IDs to the related CAPEC-ID.},
	journal = {Information},
	author = {Kanakogi, Kenta and Washizaki, Hironori and Fukazawa, Yoshiaki and Ogata, Shinpei and Okubo, Takao and Kato, Takehisa and Kanuka, Hideyuki and Hazeyama, Atsuo and Yoshioka, Nobukazu},
	month = jul,
	year = {2021},
	pages = {298},
	file = {Full Text:C\:\\Users\\glmqu\\Zotero\\storage\\LULNKF2H\\Kanakogi et al. - 2021 - Tracing CVE Vulnerability Information to CAPEC Attack Patterns Using Natural Language Processing Tec.pdf:application/pdf},
}

@misc{hu_cpe-identifier_2024,
	title = {{CPE}-{Identifier}: {Automated} {CPE} identification and {CVE} summaries annotation with {Deep} {Learning} and {NLP}},
	shorttitle = {{CPE}-{Identifier}},
	url = {http://arxiv.org/abs/2405.13568},
	doi = {10.48550/arXiv.2405.13568},
	abstract = {With the drastic increase in the number of new vulnerabilities in the National Vulnerability Database (NVD) every year, the workload for NVD analysts to associate the Common Platform Enumeration (CPE) with the Common Vulnerabilities and Exposures (CVE) summaries becomes increasingly laborious and slow. The delay causes organisations, which depend on NVD for vulnerability management and security measurement, to be more vulnerable to zero-day attacks. Thus, it is essential to come out with a technique and tool to extract the CPEs in the CVE summaries accurately and quickly. In this work, we propose the CPE-Identifier system, an automated CPE annotating and extracting system, from the CVE summaries. The system can be used as a tool to identify CPE entities from new CVE text inputs. Moreover, we also automate the data generating and labeling processes using deep learning models. Due to the complexity of the CVE texts, new technical terminologies appear frequently. To identify novel words in future CVE texts, we apply Natural Language Processing (NLP) Named Entity Recognition (NER), to identify new technical jargons in the text. Our proposed model achieves an F1 score of 95.48\%, an accuracy score of 99.13\%, a precision of 94.83\%, and a recall of 96.14\%. We show that it outperforms prior works on automated CVE-CPE labeling by more than 9\% on all metrics.},
	urldate = {2024-12-11},
	publisher = {arXiv},
	author = {Hu, Wanyu and Thing, Vrizlynn L. L.},
	month = may,
	year = {2024},
	note = {arXiv:2405.13568 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	annote = {Comment: International Conference on Information Systems Security and Privacy 2024},
	file = {Preprint PDF:C\:\\Users\\glmqu\\Zotero\\storage\\P5P6GLTS\\Hu and Thing - 2024 - CPE-Identifier Automated CPE identification and CVE summaries annotation with Deep Learning and NLP.pdf:application/pdf;Snapshot:C\:\\Users\\glmqu\\Zotero\\storage\\DK92RVF5\\2405.html:text/html},
}

@techreport{ohare_honours_2018,
	title = {Honours {Project} - {Scout}: {A} {Contactless} '{Active}' {Reconnaissance} {Known} {Vulnerability} {Assessment} {Tool}},
	shorttitle = {Honours {Project} - {Scout}},
	abstract = {Internet-wide scanning projects such as Shodan and Censys, scan the Internet and collect active reconnaissance results,and provide access to this information publicly through websites. This can be used to potentially identify systems and services which may be susceptible to known-vulnerabilities without interacting with the target devices or networks themselves.This approach can be classed ascontactless activereconnaissance.However, the vulnerability identification functionality aspect is very limited. This dissertation looks towards extending this functionality through the creation of a novel tool named‘Scout’.Through the utilization of the Censys Internet-wide scanner and the data feeds found within the National Vulnerability Database (NVD), Scout passively identifies known-vulnerabilities. This is done by the manufacturing of accurate Common Platform Enumerations (CPE) via service banner analysis of the Censys data, and associating theseto relevant Common Vulnerability and Exposures(CVE) from the NVD. Implemented through Python scripts corresponding with the Censys API and a locally stored MongoDB containing the relevant NVD data feeds. Current vulnerability assessment tools operate actively by interrogating services with specialized probes. These tools can contribute to traffic for sustained periods of time, causing disruption to the target network. Through the novel approach of Scout the problems currently associated with the alternative active approach are mitigated. Several experiments were performed. Ininitialexperimentsperformed on 2571 services across 7 local academic intuitions, 12967 known-vulnerabilities were identified. More focused experiments to evaluate results and compare with industry standard vulnerability assessment tools were carried out. Scout can successfully identify vulnerabilities which has been validated against active industry tools such as Nessus and OpenVAS. Scout has produced an effectiveness score in vulnerability identification of up to 74\% when compared to OpenVAS as a baseline.},
	author = {O'Hare, Jamie and Macfarlane, Richard},
	month = apr,
	year = {2018},
	file = {Full Text PDF:C\:\\Users\\glmqu\\Zotero\\storage\\S7XFLA37\\O'Hare and Macfarlane - 2018 - Honours Project - Scout A Contactless 'Active' Reconnaissance Known Vulnerability Assessment Tool.pdf:application/pdf},
}

@inproceedings{gawron_pvd_2017,
	title = {{PVD}: {Passive} vulnerability detection},
	shorttitle = {{PVD}},
	url = {https://ieeexplore.ieee.org/abstract/document/7921992},
	doi = {10.1109/IACS.2017.7921992},
	abstract = {The identification of vulnerabilities relies on detailed information about the target infrastructure. The gathering of the necessary information is a crucial step that requires an intensive scanning or mature expertise and knowledge about the system even though the information was already available in a different context. In this paper we propose a new method to detect vulnerabilities that reuses the existing information and eliminates the necessity of a comprehensive scan of the target system. Since our approach is able to identify vulnerabilities without the additional effort of a scan, we are able to increase the overall performance of the detection. Because of the reuse and the removal of the active testing procedures, our approach could be classified as a passive vulnerability detection. We will explain the approach and illustrate the additional possibility to increase the security awareness of users. Therefore, we applied the approach on an experimental setup and extracted security relevant information from web logs.},
	urldate = {2024-12-11},
	booktitle = {2017 8th {International} {Conference} on {Information} and {Communication} {Systems} ({ICICS})},
	author = {Gawron, Marian and Cheng, Feng and Meinel, Christoph},
	month = apr,
	year = {2017},
	keywords = {Data mining, Software, Communication systems, Databases, Monitoring, Security, Tools},
	pages = {322--327},
	file = {Full Text PDF:C\:\\Users\\glmqu\\Zotero\\storage\\3H3A242I\\Gawron et al. - 2017 - PVD Passive vulnerability detection.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\glmqu\\Zotero\\storage\\CV9VWD2R\\7921992.html:text/html},
}

@techreport{parmelee_common_2011,
	address = {NIST IR 7696},
	title = {Common {Platform} {Enumeration}: {Name} {Matching} {Specification} {Version} 2.3},
	url = {https://nvlpubs.nist.gov/nistpubs/Legacy/IR/nistir7696.pdf},
	abstract = {This report defines the Common Platform Enumeration (CPE) Name Matching version 2.3 specification. The CPE Name Matching specification is part of a stack of CPE specifications that support a variety of use cases relating to IT product description and naming. The CPE Name Matching specification provides a method for conducting a one-to-one comparison of a source CPE name to a target CPE name. In addition to defining the specification, this report also defines and explains the requirements that IT products must meet for conformance with the CPE Name Matching version 2.3 specification.},
	language = {en},
	number = {NIST IR 7696},
	institution = {National Institute of Standards and Technology},
	author = {Parmelee, Mary C and Booth, Harold and Waltermire, David and Scarfone, Karen},
	year = {2011},
	keywords = {NIST},
	file = {PDF:C\:\\Users\\glmqu\\Zotero\\storage\\EPNTDXGU\\Parmelee et al. - Common Platform Enumeration Name Matching Specification Version 2.3.pdf:application/pdf},
}

@article{noauthor_towards_2024,
	title = {Towards the application of recommender systems to secure coding},
	url = {https://www.researchgate.net/publication/333766504_Towards_the_application_of_recommender_systems_to_secure_coding},
	doi = {10.1186/s13635-019-0092-4},
	abstract = {PDF {\textbar} Secure coding is crucial for the design of secure and efficient software and computing systems. However, many programmers avoid secure coding... {\textbar} Find, read and cite all the research you need on ResearchGate},
	language = {en},
	urldate = {2024-12-11},
	journal = {ResearchGate},
	month = dec,
	year = {2024},
	file = {Full Text:C\:\\Users\\glmqu\\Zotero\\storage\\V9ZTYKNJ\\2024 - (PDF) Towards the application of recommender systems to secure coding.pdf:application/pdf;Snapshot:C\:\\Users\\glmqu\\Zotero\\storage\\PPCS6LD3\\333766504_Towards_the_application_of_recommender_systems_to_secure_coding.html:text/html},
}

@inproceedings{franco_mentor_2019,
	title = {{MENTOR}: {The} {Design} and {Evaluation} of a {Protection} {Services} {Recommender} {System}},
	shorttitle = {{MENTOR}},
	url = {https://ieeexplore.ieee.org/document/9012686},
	doi = {10.23919/CNSM46954.2019.9012686},
	abstract = {Cyberattacks are the cause of several damages on governments and companies in the last years. Such damage includes not only leaks of sensitive information, but also economic loss due to downtime of services. The security market size worth billions of dollars, which represents investments to acquire protection services and training response teams to operate such services, determines a considerable part of the investment in technologies around the world. Although a vast number of protection services are available, it is neither trivial for network operators nor endusers to choose one of them in order to prevent or mitigate an imminent attack. As the next-generation cybersecurity solutions are on the horizon, systems that simplify their adoption are still required in support of security management tasks. Thus, this paper introduces MENTOR, a support tool for cybersecurity, focusing on the recommendation of protection services. MENTOR is able to (a) to deal with different demands from the user and (b) to recommend the adequate protection service in order to provide a proper level of cybersecurity in different scenarios. Four similarity measurements are implemented in order to prove the feasibility of the MENTOR's enngine. Aug evaluation determines the performance and accuracy of each measurement used during the recommendation process.},
	urldate = {2024-12-11},
	booktitle = {2019 15th {International} {Conference} on {Network} and {Service} {Management} ({CNSM})},
	author = {Franco, Muriel Figueredo and Rodrigues, Bruno and Stiller, Burkhard},
	month = oct,
	year = {2019},
	note = {ISSN: 2165-963X},
	keywords = {Computer crime, Data mining, Tools, Cybersecurity, Engines, Protection Services, Recommender System, Recommender systems},
	pages = {1--7},
	file = {Full Text PDF:C\:\\Users\\glmqu\\Zotero\\storage\\RW5QCPR4\\Franco et al. - 2019 - MENTOR The Design and Evaluation of a Protection Services Recommender System.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\glmqu\\Zotero\\storage\\INQN8EAQ\\9012686.html:text/html},
}

@inproceedings{abuhussein_cssr_2016,
	title = {{CSSR}: {Cloud} {Services} {Security} {Recommender}},
	shorttitle = {{CSSR}},
	url = {https://ieeexplore.ieee.org/document/7557392},
	doi = {10.1109/SERVICES.2016.13},
	abstract = {The emerging paradigm of cloud computing (CC) presents many security risks that can potentially and adversely impact any one of the plethora of stakeholders. The widespread deployment and service models of CC in addition to the wide variety of stakeholders make it difficult to comprehend security and privacy (S\&P). In this paper, we present CSSR1, a Cloud Services Security Recommender tool. CSSR codifies a stakeholder-oriented taxonomy. The goal for CSSR is to identify the various S\&P risks for the kaleidoscope of different CC models from the stakeholder's perspective. CSSR will recommend a comprehensive list of S\&P attributes that must be considered as controls necessary to minimize the CC attack surface. By identifying the S\&P concerns that are unique to the particular usage scenarios (again from a stakeholder perspective), CSSR provides a comprehensive basis from which to choose alternative security solutions. This model then provides a structured and well-informed process of mitigating risk as envisioned by each and every stakeholder based on their needs.},
	urldate = {2024-12-11},
	booktitle = {2016 {IEEE} {World} {Congress} on {Services} ({SERVICES})},
	author = {Abuhussein, Abdullah and Shiva, Sajjan and Sheldon, Frederick T.},
	month = jun,
	year = {2016},
	keywords = {Security, Biological system modeling, cloud computing, Cloud computing, cloud computing security, cloud economics, Industries, service computing, Stakeholders, Standards, Taxonomy},
	pages = {48--55},
	file = {Full Text PDF:C\:\\Users\\glmqu\\Zotero\\storage\\GY4625B7\\Abuhussein et al. - 2016 - CSSR Cloud Services Security Recommender.pdf:application/pdf},
}

@inproceedings{le_using_2011,
	title = {Using {Natural} {Language} {Tool} to {Assist} {VPRG} {Automated} {Extraction} from {Textual} {Vulnerability} {Description}},
	url = {https://ieeexplore.ieee.org/document/5763565},
	doi = {10.1109/WAINA.2011.56},
	abstract = {This paper presents an application of Natural Language Tool (NLT) to support the VPRG extraction of text based vulnerability description. The NLT is used to analyze the text-based vulnerability descriptions to retrieve vulnerability properties and evaluate their relationships. Then, a graph based VPRG model that describes the vulnerability can be established. Finally, with fine-tuning from domain expertise, the VPRG model can be useful for analyzing and evaluating web-based vulnerabilities. This approach is proposed to support automatic VPRG extraction from several online database resources as well as vulnerability analysis.},
	urldate = {2024-12-11},
	booktitle = {2011 {IEEE} {Workshops} of {International} {Conference} on {Advanced} {Information} {Networking} and {Applications}},
	author = {Le, Ha Thanh and Loh, Peter Kok Keong},
	month = mar,
	year = {2011},
	keywords = {Data mining, Software, Databases, Analytical models, natural, Natural language processing, Text processing, VPRG model, Vulnerability description},
	pages = {586--592},
	file = {Full Text PDF:C\:\\Users\\glmqu\\Zotero\\storage\\K8GGJSKF\\Le and Loh - 2011 - Using Natural Language Tool to Assist VPRG Automated Extraction from Textual Vulnerability Descripti.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\glmqu\\Zotero\\storage\\5Q4KIVSB\\5763565.html:text/html},
}

@inproceedings{mcclanahan_automatically_2020,
	title = {Automatically {Locating} {Mitigation} {Information} for {Security} {Vulnerabilities}},
	url = {https://ieeexplore.ieee.org/document/9303019},
	doi = {10.1109/SmartGridComm47815.2020.9303019},
	abstract = {Software vulnerabilities pose significant security risks to systems. Usually patching can fix vulnerabilities, but patches are not always available, and in many cases patching is not preferred due to high overhead and potential service interruptions which is especially true for the electric industry. Then, other mitigation strategies are needed to mitigate security vulnerabilities. Information about mitigation strategies can be difficult to find and is typically only reported on vendor or third-party websites. In the current practice, such information is manually located by security operators, which induces high delays and operation cost. We consider this problem within the electric industry, which has particular importance and challenges because of its regulatory requirements. We propose that providing electric utilities with automatically-located mitigation information will help them overcome the time burden and mitigate vulnerabilities more timely. In particular, we develop three methods for automatically retrieving mitigation information from vendor or third-party websites. Experiment results show high performance with all the three methods.},
	urldate = {2024-12-11},
	booktitle = {2020 {IEEE} {International} {Conference} on {Communications}, {Control}, and {Computing} {Technologies} for {Smart} {Grids} ({SmartGridComm})},
	author = {McClanahan, Kylie and Li, Qinghua},
	month = nov,
	year = {2020},
	keywords = {Software, Security, automation, Computers, Conferences, mitigation, Power industry, security, Smart grids, Uniform resource locators, vulnerability},
	pages = {1--7},
	file = {Full Text PDF:C\:\\Users\\glmqu\\Zotero\\storage\\BA6MHJ7N\\McClanahan and Li - 2020 - Automatically Locating Mitigation Information for Security Vulnerabilities.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\glmqu\\Zotero\\storage\\2ELLS2LC\\9303019.html:text/html},
}

@inproceedings{zhang_dynamic_2020,
	title = {Dynamic {Risk}-{Aware} {Patch} {Scheduling}},
	url = {https://ieeexplore.ieee.org/document/9162225},
	doi = {10.1109/CNS48642.2020.9162225},
	abstract = {Every month, many new software vulnerabilities are discovered and published which will pose security risks to power grid systems if they are exploited by attackers. Thus the vulnerabilities must be patched in a timely manner to reduce the chance of being exploited. However, not all vulnerabilities can be patched quickly due to limited security resources at many electric utility companies. This paper studies dynamic risk-aware patch scheduling to determine the order of patching vulnerabilities and minimize the security risk brought by vulnerabilities. We first predict the dynamic probability of exploit over time for each vulnerability and define a metric to compute the vulnerability's dynamic risk based on the predicted probability. We then formulate two patch scheduling approaches. Evaluations on real datasets show high accuracy in predicting the dynamic probability of exploit and high effectiveness of our solutions in risk reduction compared with other scheduling methods.},
	urldate = {2024-12-11},
	booktitle = {2020 {IEEE} {Conference} on {Communications} and {Network} {Security} ({CNS})},
	author = {Zhang, Fengli and Li, Qinghua},
	month = jun,
	year = {2020},
	keywords = {Security, Power industry, Dynamic scheduling, Feature extraction, Measurement, patching, Processor scheduling, risk, scheduling, Vulnerability},
	pages = {1--9},
	file = {Full Text PDF:C\:\\Users\\glmqu\\Zotero\\storage\\3ITCVU6M\\Zhang and Li - 2020 - Dynamic Risk-Aware Patch Scheduling.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\glmqu\\Zotero\\storage\\ZARHJXSY\\9162225.html:text/html},
}

@article{ghaffarian_software_2017,
	title = {Software {Vulnerability} {Analysis} and {Discovery} {Using} {Machine}-{Learning} and {Data}-{Mining} {Techniques}: {A} {Survey}},
	volume = {50},
	issn = {0360-0300},
	shorttitle = {Software {Vulnerability} {Analysis} and {Discovery} {Using} {Machine}-{Learning} and {Data}-{Mining} {Techniques}},
	url = {https://dl.acm.org/doi/10.1145/3092566},
	doi = {10.1145/3092566},
	abstract = {Software security vulnerabilities are one of the critical issues in the realm of computer security. Due to their potential high severity impacts, many different approaches have been proposed in the past decades to mitigate the damages of software vulnerabilities. Machine-learning and data-mining techniques are also among the many approaches to address this issue. In this article, we provide an extensive review of the many different works in the field of software vulnerability analysis and discovery that utilize machine-learning and data-mining techniques. We review different categories of works in this domain, discuss both advantages and shortcomings, and point out challenges and some uncharted territories in the field.},
	number = {4},
	urldate = {2024-12-11},
	journal = {ACM Comput. Surv.},
	author = {Ghaffarian, Seyed Mohammad and Shahriari, Hamid Reza},
	month = aug,
	year = {2017},
	pages = {56:1--56:36},
	file = {Full Text PDF:C\:\\Users\\glmqu\\Zotero\\storage\\2TAM66FU\\Ghaffarian and Shahriari - 2017 - Software Vulnerability Analysis and Discovery Using Machine-Learning and Data-Mining Techniques A S.pdf:application/pdf},
}

@inproceedings{zhang_machine_2020,
	title = {A {Machine} {Learning}-based {Approach} for {Automated} {Vulnerability} {Remediation} {Analysis}},
	url = {https://ieeexplore.ieee.org/document/9162309},
	doi = {10.1109/CNS48642.2020.9162309},
	abstract = {Security vulnerabilities in firmware/software pose an important threat ton power grid security, and thus electric utility companies should quickly decide how to remediate vulnerabilities after they are discovered. Making remediation decisions is a challenging task in the electric industry due to the many factors to consider, the balance to maintain between patching and service reliability, and the large amount of vulnerabilities to deal with. Unfortunately, remediation decisions are current manually made which take a long time. This increases security risks and incurs high cost of vulnerability management. In this paper, we propose a machine learning-based automation framework to automate remediation decision analysis for electric utilities. We apply it to an electric utility and conduct extensive experiments over two real operation datasets obtained from the utility. Results show the high effectiveness of the solution.},
	urldate = {2024-12-11},
	booktitle = {2020 {IEEE} {Conference} on {Communications} and {Network} {Security} ({CNS})},
	author = {Zhang, Fengli and Huff, Philip and McClanahan, Kylie and Li, Qinghua},
	month = jun,
	year = {2020},
	keywords = {Security, automation, Power industry, security, Automation, machine learning, Machine learning, Organizations, power grid, Power grids, Standards organizations, vulnerability and patch management},
	pages = {1--9},
	file = {Full Text PDF:C\:\\Users\\glmqu\\Zotero\\storage\\7WIIVLJ7\\Zhang et al. - 2020 - A Machine Learning-based Approach for Automated Vulnerability Remediation Analysis.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\glmqu\\Zotero\\storage\\H6IWXYAV\\9162309.html:text/html},
}

@misc{noauthor_four_2011,
	title = {Four {New} {Reports} {Update} {Security} {Content} {Automation} {Protocol}},
	url = {https://www.nist.gov/news-events/news/2011/09/four-new-reports-update-security-content-automation-protocol},
	abstract = {Bringing order and security to the patchwork quilt of computing environments in a large organization can be a daunting task. Software tools and technical specifications that allow security information to be shared between information systems—the Security Content Automation Protocol (SCAP)—can save time and improve security. The National Institute of Standards and Technology (NIST) recently released four new publications that detail specifications to be used by the latest version of SCAP.

"A primary goal of automated security in a large organization's computer environment is to make sure everything is configured securely as required by management, and that all patches are applied to eliminate known vulnerabilities," said computer scientist David Waltermire. SCAP-enabled tools can scan computer systems to reveal software vulnerabilities and security configuration problems to be corrected.

SCAP relies on a fundamental component called Common Platform Enumeration (CPE), which is a standardized method of describing and identifying classes of applications, operating systems and hardware devices in an organization's computer systems. A new version of CPE has been released—version 2.3—and the four new NIST Interagency Reports (NISTIRs) provide specifications for this version, which will be used with the new SCAP version.

For SCAP to work, CPE needs to have a unique name to identify all of the same types of products. For example, without CPE, different terms, such as "Windows XP" and "Win XP," typically are used to refer to a single type of product, which can cause confusion and waste resources. CPE provides a single standardized unique name that covers all of these variants. NISTIR 7695 defines and explains the naming specification for CPE version 2.3.

Once a unique name is defined, CPE needs to compare names to determine whether they refer to some or all of the same products or platforms. For example, a product may have a unique name, but as in the Windows XP example, there may be subsets such as "Service Pack 1" or "Service Pack 2" that may further distinguish types of products. NISTIR 7696 provides the CPE name matching specification, which defines procedures for comparing two CPE names.

A dictionary specification for CPE is defined in NISTIR 7697, which includes the semantics of its data model and the rules associated with the CPE dictionary creation and management. NIST hosts the official CPE dictionary at http://nvd.nist.gov/cpe.cfm so organizations can search for and find identifier names.

With the naming, name matching and dictionary specifications defined, researchers moved to language specifications. NISTIR 7698 provides the applicability language specification, which allows construction of logical expressions built from CPE names. These expressions can be used by SCAP to identify more complex vulnerability and configuration situations, such as a problem that only exists when two applications are running together or an application is running on particular computing platforms. A real-life example is writing an applicability language expression that tells SCAP to search for situations in which Adobe Flash player version 10.3 or earlier is running on Mac OSX, Linux, Sun Solaris or Microsoft Windows.

A new publication announcing SCAP Version 1.2 is expected to be published soon. For more information on SCAP and other security automation projects, see scap.nist.gov.},
	month = sep,
	year = {2011},
	keywords = {NIST},
}

@techreport{waltermire_technical_2018,
	address = {Gaithersburg, MD},
	title = {The technical specification for the security content automation protocol ({SCAP}) version 1.3},
	url = {http://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-126r3.pdf},
	abstract = {The Security Content Automation Protocol (SCAP) is a suite of specifications that standardize the format and nomenclature by which software flaw and security configuration information is communicated, both to machines and humans. This publication, along with its annex (NIST Special Publication 800-126A) and a set of schemas, collectively define the technical composition of SCAP version 1.3 in terms of its component specifications, their interrelationships and interoperation, and the requirements for SCAP content.},
	language = {en},
	number = {NIST SP 800-126r3},
	urldate = {2024-12-10},
	institution = {National Institute of Standards and Technology},
	author = {Waltermire, David and Quinn, Stephen and Booth, Harold and Scarfone, Karen and Prisaca, Dragos},
	month = feb,
	year = {2018},
	doi = {10.6028/NIST.SP.800-126r3},
	pages = {NIST SP 800--126r3},
	file = {PDF:C\:\\Users\\glmqu\\Zotero\\storage\\8N3G78ZN\\Waltermire et al. - 2018 - The technical specification for the security content automation protocol (SCAP) version 1.3.pdf:application/pdf},
}

@techreport{wunder_specification_2011,
	address = {Gaithersburg, MD},
	title = {Specification for asset identification 1.1},
	url = {https://nvlpubs.nist.gov/nistpubs/Legacy/IR/nistir7693.pdf},
	abstract = {Asset identification plays an important role in an organization‟s ability to quickly correlate different sets of information about assets. This specification provides the necessary constructs to uniquely identify assets based on known identifiers and/or known information about the assets. This specification describes the purpose of asset identification, a data model for identifying assets, methods for identifying assets, and guidance on how to use asset identification. It also identifies a number of known use cases for asset identification.},
	language = {en},
	number = {NIST IR 7693},
	urldate = {2024-12-10},
	institution = {National Institute of Standards and Technology},
	author = {Wunder, John and Halbardier, Adam and Waltermire, David},
	year = {2011},
	doi = {10.6028/NIST.IR.7693},
	note = {Edition: 0},
	pages = {NIST IR 7693},
	file = {PDF:C\:\\Users\\glmqu\\Zotero\\storage\\KVR5GA4M\\Wunder et al. - 2011 - Specification for asset identification 1.1.pdf:application/pdf},
}

@techreport{waltermire_common_2011,
	address = {Gaithersburg, MD},
	title = {Common platform enumeration : applicability language specification version 2.3},
	shorttitle = {Common platform enumeration},
	url = {https://nvlpubs.nist.gov/nistpubs/Legacy/IR/nistir7698.pdf},
	abstract = {This report defines the Common Platform Enumeration (CPE) Applicability Language version 2.3 specification. The CPE Applicability Language specification is part of a stack of CPE specifications that support a variety of use cases relating to IT product description and naming. The CPE Applicability Language data model builds on top of other CPE specifications to provide the functionality required to allow CPE users to construct complex groupings of CPE names to describe IT platforms. These groupings are referred to as applicability statements because they are used to designate which platforms particular guidance, policies, etc. apply to. This report defines the semantics of the CPE Applicability Language data model and the requirements that IT products and CPE Applicability Language documents must meet for conformance with the CPE Applicability Language version 2.3 specification.},
	language = {en},
	number = {NIST IR 7698},
	urldate = {2024-12-10},
	institution = {National Institute of Standards and Technology},
	author = {Waltermire, David and Cichonski, Paul and Scarfone, Karen},
	year = {2011},
	doi = {10.6028/NIST.IR.7698},
	note = {Edition: 0},
	pages = {NIST IR 7698},
	file = {PDF:C\:\\Users\\glmqu\\Zotero\\storage\\CWTYYSEW\\Waltermire et al. - 2011 - Common platform enumeration  applicability language specification version 2.3.pdf:application/pdf},
}

@techreport{cichonski_common_2011,
	address = {Gaithersburg, MD},
	title = {Common platform enumeration : dictionary specification version 2.3},
	shorttitle = {Common platform enumeration},
	url = {https://nvlpubs.nist.gov/nistpubs/Legacy/IR/nistir7697.pdf},
	abstract = {This report defines the Common Platform Enumeration (CPE) Dictionary version 2.3 specification. The CPE Dictionary Specification is a part of a stack of CPE specifications that support a variety of use cases relating to information technology (IT) product description and naming. An individual CPE dictionary is a repository of IT product names, with each name in the repository identifying a unique class of IT product in the world. This specification defines the semantics of the CPE Dictionary data model and the rules associated with CPE dictionary creation and management. This report also defines and explains the requirements that IT products and services, including CPE dictionaries, must meet for conformance with the CPE Dictionary version 2.3 specification.},
	language = {en},
	number = {NIST IR 7697},
	urldate = {2024-12-10},
	institution = {National Institute of Standards and Technology},
	author = {Cichonski, Paul and Waltermire, David and Scarfone, Karen},
	year = {2011},
	doi = {10.6028/NIST.IR.7697},
	note = {Edition: 0},
	pages = {NIST IR 7697},
	file = {PDF:C\:\\Users\\glmqu\\Zotero\\storage\\AXQ8WTU9\\Cichonski et al. - 2011 - Common platform enumeration  dictionary specification version 2.3.pdf:application/pdf},
}

@techreport{cheikes_common_2011,
	address = {Gaithersburg, MD},
	title = {Common platform enumeration : naming specification version 2.3},
	shorttitle = {Common platform enumeration},
	url = {https://nvlpubs.nist.gov/nistpubs/Legacy/IR/nistir7695.pdf},
	abstract = {This report defines the Common Platform Enumeration (CPE) Naming version 2.3 specification. The CPE Naming specification is a part of a stack of CPE specifications that support a variety of use cases relating to IT product description and naming. The CPE Naming specification defines the logical structure of names for IT product classes and the procedures for binding and unbinding these names to and from machine-readable encodings. This report also defines and explains the requirements that IT products must meet for conformance with the CPE Naming version 2.3 specification.},
	language = {en},
	number = {NIST IR 7695},
	urldate = {2024-12-10},
	institution = {National Institute of Standards and Technology},
	author = {Cheikes, Brant A and Waltermire, David and Scarfone, Karen},
	year = {2011},
	doi = {10.6028/NIST.IR.7695},
	note = {Edition: 0},
	pages = {NIST IR 7695},
	file = {PDF:C\:\\Users\\glmqu\\Zotero\\storage\\HHV895RK\\Cheikes et al. - 2011 - Common platform enumeration  naming specification version 2.3.pdf:application/pdf},
}

@inproceedings{jiang_evaluating_2021,
	address = {New York, NY, USA},
	series = {{ARES} '21},
	title = {Evaluating the {Data} {Inconsistency} of {Open}-{Source} {Vulnerability} {Repositories}},
	isbn = {978-1-4503-9051-4},
	url = {https://dl.acm.org/doi/10.1145/3465481.3470093},
	doi = {10.1145/3465481.3470093},
	abstract = {Modern security practices promote quantitative methods to provide prioritisation insights and support predictive analysis, which is supported by open-source cybersecurity databases such as the Common Vulnerabilities and Exposures (CVE), the National Vulnerability Database (NVD), CERT, and vendor websites. These public repositories provide a way to standardise and share up-to-date vulnerability information, with the purpose to enhance cybersecurity awareness. However, data quality issues of these vulnerability repositories may lead to incorrect prioritisation and misemployment of resources. In this paper, we aim to empirically analyse the data quality impact of vulnerability repositories for actual information technology (IT) and operating technology (OT) systems, especially on data inconsistency. Our case study shows that data inconsistency may misdirect investment of cybersecurity resources. Instead, correlated vulnerability repositories and trustworthiness data verification bring substantial benefits for vulnerability management.},
	urldate = {2024-12-10},
	booktitle = {Proceedings of the 16th {International} {Conference} on {Availability}, {Reliability} and {Security}},
	publisher = {Association for Computing Machinery},
	author = {Jiang, Yuning and Jeusfeld, Manfred and Ding, Jianguo},
	month = aug,
	year = {2021},
	pages = {1--10},
	file = {Full Text PDF:C\:\\Users\\glmqu\\Zotero\\storage\\XZ4JHYKX\\Jiang et al. - 2021 - Evaluating the Data Inconsistency of Open-Source Vulnerability Repositories.pdf:application/pdf},
}

@inproceedings{mcclanahan_towards_2024,
	address = {Big Island, HI, USA},
	title = {Towards {Automatically} {Matching} {Security} {Advisories} to {CPEs}: {String} {Similarity}-based {Vendor} {Matching}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-7099-7},
	shorttitle = {Towards {Automatically} {Matching} {Security} {Advisories} to {CPEs}},
	url = {https://ieeexplore.ieee.org/document/10556231/},
	doi = {10.1109/ICNC59896.2024.10556231},
	abstract = {When a vulnerability is reported by the National Vulnerability Database (NVD), affected products are listed in the structured Common Platform Enumeration (CPE) format. Unfortunately, if the vulnerability is in a software library (e.g., Log4j), it will not include CPEs for each product containing that library. In these cases, security operators need to manually read the vendor’s or third-party security advisories to see if their product is affected. However, these advisories do not report affected products in a structured format, which prevents automated processing.},
	language = {en},
	urldate = {2024-12-10},
	booktitle = {2024 {International} {Conference} on {Computing}, {Networking} and {Communications} ({ICNC})},
	publisher = {IEEE},
	author = {McClanahan, Kylie and Li, Qinghua},
	month = feb,
	year = {2024},
	pages = {233--238},
	file = {PDF:C\:\\Users\\glmqu\\Zotero\\storage\\WHXCZYJY\\McClanahan and Li - 2024 - Towards Automatically Matching Security Advisories to CPEs String Similarity-based Vendor Matching.pdf:application/pdf},
}

@inproceedings{huff_recommender_2021,
	address = {New York, NY, USA},
	series = {{ARES} '21},
	title = {A {Recommender} {System} for {Tracking} {Vulnerabilities}},
	isbn = {978-1-4503-9051-4},
	url = {https://dl.acm.org/doi/10.1145/3465481.3470039},
	doi = {10.1145/3465481.3470039},
	abstract = {Mitigating vulnerabilities in software requires first identifying the vulnerabilities with an organization’s software assets. This seemingly trivial task involves maintaining vendor product vulnerability notification for a kludge of hardware and software packages from innumerable software publishers, coding projects, and third-party package managers. On the other hand, software vulnerability databases are often consistently reported and categorized in clean, standard formats and neatly tied to a common software product enumerator (i.e., CPE). Currently it is a heavy workload for cybersecurity analysts at organizations to match their hardware and software package inventory to target CPEs. This hinders organizations from getting notifications for new vulnerabilities, and identifying applicable vulnerabilities. In this paper, we present a recommender system to automatically identify a minimal candidate set of CPEs for software names to improve vulnerability identification and alerting accuracy. The recommender system uses a pipeline of natural language processing, fuzzy matching, and machine learning to significantly reduce the human effort needed for software product vulnerability matching.},
	urldate = {2024-12-10},
	booktitle = {Proceedings of the 16th {International} {Conference} on {Availability}, {Reliability} and {Security}},
	publisher = {Association for Computing Machinery},
	author = {Huff, Philip and McClanahan, Kylie and Le, Thao and Li, Qinghua},
	month = aug,
	year = {2021},
	pages = {1--7},
	file = {Full Text PDF:C\:\\Users\\glmqu\\Zotero\\storage\\Q4R2EMF6\\Huff et al. - 2021 - A Recommender System for Tracking Vulnerabilities.pdf:application/pdf},
}

@inproceedings{ushakov_cpe_2021,
	title = {{CPE} and {CVE} based {Technique} for {Software} {Security} {Risk} {Assessment}},
	volume = {1},
	url = {https://ieeexplore.ieee.org/abstract/document/9660968},
	doi = {10.1109/IDAACS53288.2021.9660968},
	abstract = {Currently, a lot of work has been done in the area of detection, scoring, and inventory of software and hardware vulnerabilities. Known vulnerabilities are listed in the open databases. It is essential to continuously monitor that information system doesn't contain severe vulnerabilities to ensure its information security. Applicability of open vulnerability databases is limited by the challenges occurring due to automated mapping the software product names in the analyzed system logs to their product names in the open sources (to extract relevant vulnerabilities from them). The paper proposes the technique incorporating an algorithm for mapping the software products names in the analyzed system logs to the relevant Common Platform Enumeration entries in open vulnerability databases based on the Ratcliff/Obershelp algorithm, identification of known vulnerabilities for the detected entries, and security risk assessment of the analysed system. The technique is implemented and tested using Windows computers software and has shown an accuracy of 79\% on average.},
	urldate = {2024-12-10},
	booktitle = {2021 11th {IEEE} {International} {Conference} on {Intelligent} {Data} {Acquisition} and {Advanced} {Computing} {Systems}: {Technology} and {Applications} ({IDAACS})},
	author = {Ushakov, Roman and Doynikova, Elena and Novikova, Evgenia and Kotenko, Igor},
	month = sep,
	year = {2021},
	note = {ISSN: 2770-4254},
	keywords = {Software, Databases, Conferences, CPE, CVE, cybersecurity, Data acquisition, databases, Hardware, Information security, mapping, products, risk assessment, software, Software algorithms, vulnerabilities},
	pages = {353--356},
	file = {Full Text PDF:C\:\\Users\\glmqu\\Zotero\\storage\\JAASDVK4\\Ushakov et al. - 2021 - CPE and CVE based Technique for Software Security Risk Assessment.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\glmqu\\Zotero\\storage\\H97KR85K\\9660968.html:text/html},
}

@misc{sanguino_software_2017,
	title = {Software {Vulnerability} {Analysis} {Using} {CPE} and {CVE}},
	url = {http://arxiv.org/abs/1705.05347},
	doi = {10.48550/arXiv.1705.05347},
	abstract = {In this paper, we analyze the Common Platform Enumeration (CPE) dictionary and the Common Vulnerabilities and Exposures (CVE) feeds. These repositories are widely used in Vulnerability Management Systems (VMSs) to check for known vulnerabilities in software products. The analysis shows, among other issues, a lack of synchronization between both datasets that can lead to incorrect results output by VMSs relying on those datasets. To deal with these problems, we developed a method that recommends to a user a prioritized list of CPE identifiers for a given software product. The user can then assign (and, if necessary, adapt) the most suitable CPE identifier to the software so that regular (e.g., daily) checks can find known vulnerabilities for this software in the CVE feeds. Our evaluation of this method shows that this interaction is indeed necessary because a fully automated CPE assignment is prone to errors due to the CPE and CVE shortcomings. We implemented an open-source VMS that employs the proposed method and published it on GitHub.},
	urldate = {2024-12-10},
	publisher = {arXiv},
	author = {Sanguino, Luis Alberto Benthin and Uetz, Rafael},
	month = may,
	year = {2017},
	note = {arXiv:1705.05347 [cs]},
	keywords = {Computer Science - Cryptography and Security},
	file = {Preprint PDF:C\:\\Users\\glmqu\\Zotero\\storage\\H6KXRFCF\\Sanguino and Uetz - 2017 - Software Vulnerability Analysis Using CPE and CVE.pdf:application/pdf;Snapshot:C\:\\Users\\glmqu\\Zotero\\storage\\G35XKI5S\\1705.html:text/html},
}

@article{robertson_probabilistic_2009,
	title = {The {Probabilistic} {Relevance} {Framework}: {BM25} and {Beyond}},
	volume = {3},
	shorttitle = {The {Probabilistic} {Relevance} {Framework}},
	doi = {10.1561/1500000019},
	abstract = {The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970—1980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters.},
	journal = {Foundations and Trends in Information Retrieval},
	author = {Robertson, Stephen and Zaragoza, Hugo},
	month = jan,
	year = {2009},
	pages = {333--389},
	file = {Full Text PDF:C\:\\Users\\glmqu\\Zotero\\storage\\SVGUQ9UY\\Robertson and Zaragoza - 2009 - The Probabilistic Relevance Framework BM25 and Beyond.pdf:application/pdf},
}

@inproceedings{zaragoza_microsoft_2004,
	title = {Microsoft {Cambridge} at {TREC} 13: {Web} and {Hard} {Tracks}.},
	shorttitle = {Microsoft {Cambridge} at {TREC} 13},
	author = {Zaragoza, Hugo and Craswell, Nick and Taylor, Michael and Saria, Suchi and Robertson, Stephen},
	month = jan,
	year = {2004},
	file = {Full Text PDF:C\:\\Users\\glmqu\\Zotero\\storage\\T3DNCLDU\\Zaragoza et al. - 2004 - Microsoft Cambridge at TREC 13 Web and Hard Tracks..pdf:application/pdf},
}

@article{ketola_bm25-fic_nodate,
	title = {{BM25}-{FIC}: {Information} {Content}-based {Field} {Weighting} for {BM25F}},
	abstract = {BM25F has been shown to perform well on many multi-ﬁeld and multi-modal retrieval tasks. However, one of its key challenges is ﬁnding appropriate ﬁeld weights. This paper tackles the challenge by introducing a new analytical method for the automatic estimation of these weights. The method — denoted BM25-FIC — is based on ﬁeld information content (FIC), calculated from term, collection and ﬁeld statistics. The ﬁeld weights are applied to each document separately rather than to the entire ﬁeld, as normally done by BM25F where the ﬁeld weights are constant across documents. The BM25-FIC outperforms the BM25F in terms of P@10, MAP and NDCG on a small test collection. Then the paper introduces an interactive information discovery model based on the ﬁeld weights. The weights are used to compute a similarity score between a seed document and the retrieved documents. Overall, the BM25-FIC approach is an enhanced BM25F method that combines information-oriented search and parameter estimation.},
	language = {en},
	author = {Ketola, Tuomas and Roelleke, Thomas},
	file = {PDF:C\:\\Users\\glmqu\\Zotero\\storage\\JPLH8UXH\\Ketola and Roelleke - BM25-FIC Information Content-based Field Weighting for BM25F.pdf:application/pdf},
}

@article{garcia_tutorial_2016,
	title = {A {Tutorial} on the {BM25F} {Model}},
	abstract = {This is a tutorial on the Best Match 25 Model with Extension to Multiple Weighted Fields , also known as the BM25F Model. Unlike BM25, the model is applicable to structured documents consisting of multiple fields. The model preserves term frequency nonlinearity and removes the independence assumption between same term occurrences.},
	author = {Garcia, Edel},
	month = oct,
	year = {2016},
	file = {Full Text PDF:C\:\\Users\\glmqu\\Zotero\\storage\\88UIXTHE\\Garcia - 2016 - A Tutorial on the BM25F Model.pdf:application/pdf},
}

@inproceedings{robertson_simple_2004,
	address = {Washington D.C. USA},
	title = {Simple {BM25} extension to multiple weighted fields},
	isbn = {978-1-58113-874-0},
	url = {https://dl.acm.org/doi/10.1145/1031171.1031181},
	doi = {10.1145/1031171.1031181},
	abstract = {This paper describes a simple way of adapting the BM25 ranking formula to deal with structured documents. In the past it has been common to compute scores for the individual ﬁelds (e.g. title and body) independently and then combine these scores (typically linearly) to arrive at a ﬁnal score for the document. We highlight how this approach can lead to poor performance by breaking the carefully constructed non-linear saturation of term frequency in the BM25 function. We propose a much more intuitive alternative which weights term frequencies before the nonlinear term frequency saturation function is applied. In this scheme, a structured document with a title weight of two is mapped to an unstructured document with the title content repeated twice. This more verbose unstructured document is then ranked in the usual way. We demonstrate the advantages of this method with experiments on Reuters Vol1 and the TREC dotGov collection.},
	language = {en},
	urldate = {2024-12-10},
	booktitle = {Proceedings of the thirteenth {ACM} international conference on {Information} and knowledge management},
	publisher = {ACM},
	author = {Robertson, Stephen and Zaragoza, Hugo and Taylor, Michael},
	month = nov,
	year = {2004},
	pages = {42--49},
	file = {PDF:C\:\\Users\\glmqu\\Zotero\\storage\\J4IP5QQ3\\Robertson et al. - 2004 - Simple BM25 extension to multiple weighted fields.pdf:application/pdf},
}

@inproceedings{tovarnak_graph-based_2021,
	title = {Graph-{Based} {CPE} {Matching} for {Identification} of {Vulnerable} {Asset} {Configurations}},
	url = {https://ieeexplore.ieee.org/document/9463994/?arnumber=9463994},
	abstract = {In this manuscript, we propose a graph-based approach for identification of vulnerable asset configurations via Common Platform Enumeration matching. The approach consists of a graph model and insertion procedure that is able to represent and store information about CVE vulnerabilities and different configurations of CPE-classified asset components. These building blocks are accompanied with a search query in Gremlin graph traversal language that is able to find all vulnerable pairs of CVEs and asset configurations in a single traversal, as opposed to a conventional brute-force approach.},
	urldate = {2024-12-10},
	booktitle = {2021 {IFIP}/{IEEE} {International} {Symposium} on {Integrated} {Network} {Management} ({IM})},
	author = {Tovarňák, Daniel and Sadlek, Lukáš and Čeleda, Pavel},
	month = may,
	year = {2021},
	note = {ISSN: 1573-0077},
	keywords = {Software, CPE, CVE, Hardware, Common Platform Enumeration, Common Vulnerabilities and Exposures, Dictionaries, graph model, Gremlin, Mathematical model, Object recognition, Operating systems, Portable computers},
	pages = {986--991},
	file = {Full Text PDF:C\:\\Users\\glmqu\\Zotero\\storage\\73XJCRSM\\Tovarňák et al. - 2021 - Graph-Based CPE Matching for Identification of Vulnerable Asset Configurations.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\glmqu\\Zotero\\storage\\4BSUZ3LR\\9463994.html:text/html},
}

@article{noauthor_software_nodate,
	title = {Software {Identification} {Ecosystem} {Option} {Analysis}},
	language = {en},
	file = {PDF:C\:\\Users\\glmqu\\Zotero\\storage\\6KE9JA4V\\Software Identification Ecosystem Option Analysis.pdf:application/pdf;Request for Comment on Software Identification Ecosystem Option Analysis:C\:\\Users\\glmqu\\Zotero\\storage\\ZTM28BH5\\comment.html:text/html},
}

@misc{hughes_whats_2024,
	title = {What's in a name?},
	url = {https://www.resilientcyber.io/p/whats-in-a-name},
	abstract = {A look at the current software identification ecosystem},
	language = {en},
	urldate = {2025-01-22},
	author = {Hughes, Chris},
	month = mar,
	year = {2024},
	file = {Snapshot:C\:\\Users\\glmqu\\Zotero\\storage\\AE3HLAW4\\whats-in-a-name.html:text/html},
}

@article{ramirez_software_nodate,
	title = {Software {Identification} {Challenges} and {Guidance}},
	language = {en},
	author = {Ramirez, Ruben},
	file = {PDF:C\:\\Users\\glmqu\\Zotero\\storage\\SR8HUB2E\\Ramirez - NTIA Multistakeholder Process on Software Component Transparency Framing Working Group.pdf:application/pdf},
}

@article{noauthor_software_nodate-1,
	title = {Software {Identity} {Discussion} and {Guidance}},
	url = {https://www.ntia.doc.gov/files/ntia/publications/ntia_sbom_framing_sw_identity_july9.pdf},
	urldate = {2025-01-22},
	file = {PDF:C\:\\Users\\glmqu\\Zotero\\storage\\MX25BKMN\\ntia_sbom_framing_sw_identity_july9.pdf:application/pdf},
}

@article{manion_universal_nodate,
	title = {Universal ({Software}) {Product} {Identity}: {Solving} a {Hard} {Problem} {Twice} {Over}},
	language = {en},
	author = {Manion, Art and Proell, Thomas and ProductCERT, Siemens and Schmidt, Thomas},
	file = {PDF:C\:\\Users\\glmqu\\Zotero\\storage\\N2LIQ4Z9\\Manion et al. - Universal (Software) Product Identity Solving a Hard Problem Twice Over.pdf:application/pdf},
}

@misc{fontaine_comment_nodate,
	title = {Comment {Submitted} by {Hushmesh} {Inc}.},
	author = {Fontaine, Manu},
	file = {PDF:C\:\\Users\\glmqu\\Zotero\\storage\\6SYT5Q3B\\CISA-2023-0026-0013_attachment_3.pdf:application/pdf},
}

@misc{noauthor_comment_nodate,
	title = {Comment {Submitted} by {OpenSSF}},
	file = {PDF:C\:\\Users\\glmqu\\Zotero\\storage\\2HJL5NGD\\CISA-2023-0026-0014_attachment_2.pdf:application/pdf},
}

@misc{manion_comment_nodate,
	title = {Comment {Submitted} by {Art} {Manion}},
	author = {Manion, Art},
	file = {PDF:C\:\\Users\\glmqu\\Zotero\\storage\\EAH2IJ4Y\\CISA-2023-0026-0012_attachment_1.pdf:application/pdf},
}

@article{wheeler_comments_nodate,
	title = {Comments to the {CISA} {Request} for {Comment} ({RFC}) on the {Software} {Identification} {Ecosystem} {Analysis} {White} {Paper}},
	language = {en},
	author = {Wheeler, Dr David A},
	file = {PDF:C\:\\Users\\glmqu\\Zotero\\storage\\676TSBYP\\Wheeler - Comments to the CISA Request for Comment (RFC) on the Software Identification Ecosystem Analysis Whi.pdf:application/pdf},
}

@misc{noauthor_comment_nodate-1,
	title = {Comment {Submitted} by {The} {GUAC} {Project}},
	file = {PDF:C\:\\Users\\glmqu\\Zotero\\storage\\7F85I8KL\\CISA-2023-0026-0011_attachment_1.pdf:application/pdf},
}

@misc{noauthor_comment_nodate-2,
	title = {Comment {Submitted} by {Google} {LLC}},
	file = {PDF:C\:\\Users\\glmqu\\Zotero\\storage\\LEZB8MDB\\CISA-2023-0026-0010_attachment_1.pdf:application/pdf},
}

@misc{noauthor_comment_nodate-3,
	title = {Comment {Submitted} by {Synopsys} {Inc}},
	file = {PDF:C\:\\Users\\glmqu\\Zotero\\storage\\5UKNU4DY\\CISA-2023-0026-0008_attachment_1.pdf:application/pdf},
}

@misc{d_goldstick_combatting_2024,
	title = {Combatting {Supply} {Chain} {Cyber} {Threats}: {Safeguarding} {Data} and {Protecting} {Digital} {Supply} {Chains}},
	shorttitle = {Combatting {Supply} {Chain} {Cyber} {Threats}},
	url = {https://www.foley.com/insights/publications/2024/04/supply-chain-cyber-threats-safeguarding-data/},
	abstract = {As supply chains have become more digitized and interconnected, they have also become more vulnerable to a range of cyber threats.},
	language = {en-US},
	urldate = {2025-02-18},
	journal = {Foley \& Lardner LLP},
	author = {D. Goldstick, Samuel},
	month = apr,
	year = {2024},
	file = {Snapshot:C\:\\Users\\glmqu\\Zotero\\storage\\SQ5B9WTL\\supply-chain-cyber-threats-safeguarding-data.html:text/html},
}

@article{osborne_software_nodate,
	title = {Software {Supply} {Chain} {Report} 2023},
	language = {en},
	author = {Osborne, Charlie},
	file = {PDF:C\:\\Users\\glmqu\\Zotero\\storage\\VVLUAVZ6\\Software Supply Chain Report 2023.pdf:application/pdf;Snapshot:C\:\\Users\\glmqu\\Zotero\\storage\\SRMDP2IG\\software-supply-chain-attacks-to-cost-the-world-60-billion-by-2025.html:text/html},
}

@misc{juniper_vulnerable-software-supply-chains-are--multi-billion-dollar-problem_2023,
	title = {vulnerable-software-supply-chains-are-a-multi-billion-dollar-problem},
	publisher = {Juniper Research},
	author = {Juniper, Research},
	month = may,
	year = {2023},
	file = {PDF:C\:\\Users\\glmqu\\Zotero\\storage\\GQZGYADL\\vulnerable-software-supply-chains-are-a-multi-billion-dollar-problem.pdf:application/pdf;Revenue Losses Attributable to Supply Chain Cyberattack:C\:\\Users\\glmqu\\Zotero\\storage\\J5A236GR\\Revenue Losses Attributable to Supply Chain Cyberattack.png:image/png},
}

@misc{itrc_2023-annual-data-breach-report_2024,
	title = {2023-{Annual}-{Data}-{Breach}-{Report}},
	url = {https://www.idtheftcenter.org/wp-content/uploads/2024/01/ITRC_2023-Annual-Data-Breach-Report.pdf},
	urldate = {2025-02-19},
	journal = {ITRC Identity Theft Resource Center},
	author = {ITRC},
	month = jan,
	year = {2024},
	file = {PDF:C\:\\Users\\glmqu\\Zotero\\storage\\NDINUHXU\\ITRC_2023-Annual-Data-Breach-Report.pdf:application/pdf},
}

@article{noauthor_proposal_nodate,
	title = {A {Proposal} to {Operationalize} {Component} {Identification} for {Vulnerability} {Management}},
	language = {en},
	file = {PDF:C\:\\Users\\glmqu\\Zotero\\storage\\V2QHMC5G\\A Proposal to Operationalize Component Identification for Vulnerability Management.pdf:application/pdf},
}

@book{manning_introduction_2008,
	title = {Introduction to {Information} {Retrieval}},
	publisher = {Cambridge University Press},
	author = {Manning, Christopher D. and Raghavan, Prabhakar and Schütze, Hinrich},
	year = {2008},
	file = {PDF:C\:\\Users\\glmqu\\Zotero\\storage\\KY6L86WJ\\Manning et al. - 2008 - Introduction to Information Retrieval.pdf:application/pdf},
}

@article{sparck_jones_statistical_1972,
	title = {A {STATISTICAL} {INTERPRETATION} {OF} {TERM} {SPECIFICITY} {AND} {ITS} {APPLICATION} {IN} {RETRIEVAL}},
	volume = {28},
	issn = {0022-0418},
	url = {https://www.emerald.com/insight/content/doi/10.1108/eb026526/full/html},
	doi = {10.1108/eb026526},
	language = {en},
	number = {1},
	urldate = {2025-03-01},
	journal = {Journal of Documentation},
	author = {Sparck Jones, Karen},
	month = jan,
	year = {1972},
	pages = {11--21},
}
